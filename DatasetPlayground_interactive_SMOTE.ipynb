{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for the 20th century <br> Can a database from 1987 identify a glass cosmetics jar from the 1930s?\n",
    "\n",
    "Author: Joe Cerniglia <br>\n",
    "Date: March 18, 2022 <br><br>\n",
    "This analysis uses a database of glass samples from 1987 developed by ``Ian W. Evett`` and ``Ernest J. Spiehler``. Their paper, first presented at the 1987 conference of the KBS (Knowledge-Based Systems) in Goverment, is: <br>Evett, Ian W. and Spiehler, E. J., \"Rule Induction in Forensic Science.\" http://gpbib.cs.ucl.ac.uk/gp-html/evett_1987_rifs.html<br><br>\n",
    "The weight percent of eight elemental components and refractive index comprise the features of their database and each sample is represented by a target variable called Type. The types of glass represented in the sample are: Window Float, Window Non-Float, Vehicle Float, Container, Tableware, and Headlamp. There are no Vehicle Non-Float types represented in the data.\n",
    "\n",
    "I have two glass samples that were independently measured for most of these elements.* The first is a 1930s semi-opaque cosmetic jar with a circular, double-lipped pedestal base. It was recovered from the island of Nikumaroro in 2010 and has been speculated to have possibly belonged to aviation pioneer Amelia Earhart. I will refer to this jar as the ``artifact jar``. The second is a transparent cosmetic jar, purchased on eBay, of unknown manufacture date, in the same size and shape as the first. I will refer to this jar as the ``clear facsimile``. Both jars were manufactured by the Hazel-Atlas Glass Company.\n",
    "\n",
    "There are two main research questions I wish to answer:<br>\n",
    "1) What do the correlations between elements for the different types of glass in the 1987 database reveal about late 20th century glassmaking, as compared with early 20th century glassmaking?<br>\n",
    "2) Using machine learning to train a model on the 1987 database, can that model be used to identify the type (container) of one or both of the older samples unseen by the model?\n",
    "\n",
    "\\* Because silicon was measured in my lab data as 'Matrix,' with no actual number stated, I estimated the silicon for both containers based on the average for containers in the database (72.37). Because K (potassium) was measured at 980 ppm for the clear facsimile, rather than by wt%, I assigned it a value of 50% of the wt% for the artifact jar: .5 * .24 = .12. Because Fe (iron) is at very low wt% levels in the 1987 database, and the levels in my data are listed at very low parts per million (ppm), I assigned to the artifact jar and to the clear facsimile a wt% of Fe of .02 and .01, respectively. \n",
    "\n",
    "Refractive index was not measured for either the artifact jar or the clear facsimile. Since the clear facsimile is completely transparent, I assigned it the minimum refractive index of containers from the 1987 dataset. Since the artifact jar is semi-opaque, I assigned it the maximum refractive index of containers from the 1987 dataset. These educated guesses were necessary for the machine learning experiment that concludes this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a simple report describing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214 entries, 0 to 213\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      214 non-null    int64  \n",
      " 1   Ref_ix  214 non-null    float64\n",
      " 2   Na      214 non-null    float64\n",
      " 3   Mg      214 non-null    float64\n",
      " 4   Al      214 non-null    float64\n",
      " 5   Si      214 non-null    float64\n",
      " 6   K       214 non-null    float64\n",
      " 7   Ca      214 non-null    float64\n",
      " 8   Ba      214 non-null    float64\n",
      " 9   Fe      214 non-null    float64\n",
      " 10  Type    214 non-null    int64  \n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 18.5 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Ref_ix</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>107.500000</td>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>2.780374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.920648</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>2.103739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.250000</td>\n",
       "      <td>1.516522</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>107.500000</td>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>160.750000</td>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID      Ref_ix          Na          Mg          Al          Si  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean   107.500000    1.518365   13.407850    2.684533    1.444907   72.650935   \n",
       "std     61.920648    0.003037    0.816604    1.442408    0.499270    0.774546   \n",
       "min      1.000000    1.511150   10.730000    0.000000    0.290000   69.810000   \n",
       "25%     54.250000    1.516522   12.907500    2.115000    1.190000   72.280000   \n",
       "50%    107.500000    1.517680   13.300000    3.480000    1.360000   72.790000   \n",
       "75%    160.750000    1.519157   13.825000    3.600000    1.630000   73.087500   \n",
       "max    214.000000    1.533930   17.380000    4.490000    3.500000   75.410000   \n",
       "\n",
       "                K          Ca          Ba          Fe        Type  \n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  \n",
       "mean     0.497056    8.956963    0.175047    0.057009    2.780374  \n",
       "std      0.652192    1.423153    0.497219    0.097439    2.103739  \n",
       "min      0.000000    5.430000    0.000000    0.000000    1.000000  \n",
       "25%      0.122500    8.240000    0.000000    0.000000    1.000000  \n",
       "50%      0.555000    8.600000    0.000000    0.000000    2.000000  \n",
       "75%      0.610000    9.172500    0.000000    0.100000    3.000000  \n",
       "max      6.210000   16.190000    3.150000    0.510000    7.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "# Make a dictionary of types for later translation.\n",
    "# When rounding predictions, a type of 0 is sometimes generated.\n",
    "# The 0 entry in the dictionary allows for this posssibility.\n",
    "dict = {0: 'WindowF',\n",
    "  1: 'WindowF',\n",
    "  2: 'WindowNF',\n",
    "  3: 'VehicleF',\n",
    "  4: 'VehicleNF',\n",
    "  5: 'Container',\n",
    "  6: 'Tableware',\n",
    "  7: 'Headlamp'}\n",
    "# Read in the data into two datasets; one for graphing and the other\n",
    "# for a machine learning experiment.\n",
    "filename='glass.data'\n",
    "names=['ID','Ref_ix','Na','Mg','Al','Si','K','Ca','Ba','Fe','Type']\n",
    "\n",
    "dataset = read_csv(filename, header=None, sep=',',names=names, na_values='.')\n",
    "dataset_ml = read_csv(\n",
    "    filename, header=None, sep=',',names=names, na_values='.')\n",
    "\n",
    "display(dataset_ml.info())\n",
    "display(dataset_ml.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nulls in the dataset. The column types are all numeric. All 214 samples of the database have been included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are functions for graphing the count of various glass types in the 1987 database given a range of values for a single element in the periodic table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticks_restrict_to_integer(axis):\n",
    "    \"\"\"Restrict the ticks on the given axis to be at least integer,\n",
    "    that is no half ticks at 1.5 for example.\n",
    "    \"\"\"\n",
    "    from matplotlib.ticker import MultipleLocator\n",
    "    major_tick_locs = axis.get_majorticklocs()\n",
    "    if len(major_tick_locs) < 2 or major_tick_locs[1] - major_tick_locs[0] < 1:\n",
    "        axis.set_major_locator(MultipleLocator(1))    \n",
    "\n",
    "def numgroups(element,low,high):\n",
    "    \"\"\"\n",
    "    non-fruitful function. Outputs a graph if the number of types of\n",
    "    glass within the DataFrame is > 1.  Otherwise, the function prints\n",
    "    a message, for aesthetic reasons.\n",
    "\n",
    "    parameters:\n",
    "    element: an element in the periodic table\n",
    "    low: A minimum value for the measured element, which acts as the \n",
    "    minimum value to allow into the dataset sample\n",
    "    high: A maximum value for the measured element, which acts as the \n",
    "    maximum value to allow into the dataset sample\n",
    "\n",
    "    Preconditions:\n",
    "    element is a string with value of: 'Mg' or 'Ca' or 'Ba'\n",
    "    \n",
    "    Low cannnot be greater than high.\n",
    "    \"\"\"\n",
    "    if low>high:\n",
    "        print('Low value must be smaller than the high value. Try again.')\n",
    "        return\n",
    "    rangedict = {'Ba':['Barium',[.37,.74]],\n",
    "                 'Mg':['Magnesium',[2.4,4.3]],\n",
    "                 'Ca':['Calcium',[3.6,8.5]]}\n",
    "    lowref=rangedict[element][1][0]\n",
    "    highref=rangedict[element][1][1]\n",
    "    element_full_name=rangedict[element][0]\n",
    "    sampl = dataset[(dataset[element] >= low) & (\n",
    "        dataset[element] <= high)]\n",
    "    number_groups=sampl['Type'].value_counts().shape[0]\n",
    "    if number_groups==1:\n",
    "        print('There is only one sample Type')\n",
    "        print(sampl['Type'].values[0],'=',sampl.shape[0])\n",
    "        print('reference: Clear facsimile jar =',lowref,'wt%'\n",
    "              '\\n  1930s Artifact jar=',highref,'wt%')\n",
    "    else:\n",
    "        print('reference: Clear facsimile jar =',lowref,'wt%'\n",
    "              '\\n  1930s Artifact jar=',highref,'wt%')\n",
    "        try:\n",
    "            sampl['Type'].value_counts().plot(\n",
    "                kind='bar', figsize=(10, 6), rot=0, cmap='Spectral')\n",
    "            plt.xlabel(\"Glass Type\", labelpad=14,fontsize=16,rotation=0)\n",
    "            plt.ylabel(\"Count of Type\", labelpad=70, \n",
    "                       fontsize=16,rotation=0)\n",
    "            plt.xticks(fontsize=14)\n",
    "            plt.yticks(fontsize=14)\n",
    "            ax = plt.subplot()\n",
    "            ticks_restrict_to_integer(ax.yaxis)\n",
    "            plt.title(\n",
    "                \"Count of Glass Types in the 1987 database \\n with\" +\n",
    "                \" a range of of measured values of \" + \n",
    "                element_full_name + \" \\n between \" + str(low) +\n",
    "                \" and \" + str(high) + ' wt%' , y=1.02,fontsize=16)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print('No glass samples are in the range of ' + str(low) +\n",
    "                \" and \" + str(high) + ' wt%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What types of glass in the 1987 database are most similar to the older samples in terms of magnesium, barium or calcium content?\n",
    "Choose from the drop down list below to select one of three elements, magnesium (Mg), barium (Ba) or calcium (Ca), from the periodic table. Each of these elements is an important ingredient in glassmaking, influencing the final appearance of the glass product.<br> Slide the sliders to select a range of values for the element you selected.<br>\n",
    "Run the graph by pushing the button labeled ``Run Interact``. Once you have run the graph, take a look at the supplied reference values of elemental weight percent from the artifact samples. With the sliders, select a range that closely fits the reference values, and you will receive a precise comparison of how the 1987 database compares with the artifact samples. Set the low slider to 0 and the high slider to maximum to see the distribution of the database as a whole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f61d45e2a74e86a1d82b00ffc5c036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='element', options=('Mg', 'Ca', 'Ba'), value='Mg'), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact_manual\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset.replace({\"Type\": dict},inplace=True)\n",
    "ELEMENTS = ['Mg','Ca','Ba']\n",
    "interact_manual(numgroups, element=ELEMENTS, low=(0, \n",
    "        dataset[ELEMENTS].max().max(), 0.01), \n",
    "        high=(0, dataset[ELEMENTS].max().max(), 0.01),\n",
    "                continuous_update=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Magnesium:<br>\n",
    "The clear facsimile jar has 2.4 wt% Mg. Based on the fact that there is only one sample type, tableware, with values between 2.29 and 2.40 wt%, the Mg (were it the only feature) would seem to predict the clear facsimile jar to be in the tableware family, a close cousin to the container family. But there are numerous other windows and even one container directly on either side of these parameters.<br>The artifact jar has 4.3 wt% Mg. This is nearly off-scale high, a value above all others in the database, save one. There is a single float window with 4.49 wt% Mg in the database. Any Mg value above 3.5 wt% is likely to be a window, and not a container.[1]\n",
    "\n",
    "Calcium:<br>\n",
    "The clear facsimile jar has 3.6 wt% Ca. This is off-scale low, a value less than all the samples in the database. The next-highest value, 5.43 wt% Ca, is a headlamp.<br>The artifact jar has 8.5 wt% Ca. This is close to the mean in the database, but the mean of containers is higher. Twenty-four (24) windows are associated with this value, as are six (6) headlamps and four (4) vehicle windshields. Containers typically have higher values of Ca, as is shown in the cut of the data in the next section.\n",
    "\n",
    "Barium:<br>\n",
    "The ratio of 2:1 for the element Barium in the two glass samples (.74 for the artifact and .37 for the clear facsimile jar) suggests that the glass maker, Hazel-Atlas, used a recipe. This is not uncommon in the glass industry. One window and one headlamp are associated with values of Ba between .25 and .43 wt%. Two headlamps and one window are associated with values of Ba between .69 and .87 wt%.<br><br>Conclusion: We have not yet applied any machine learning to the identification of the unseen samples, but it would appear from the weight percent of some of the key ingredients from these samples that windows, of the float or non-float variety, and, to a lesser extent, headlamps, are strong candidates for how the 1987 database might predict their identity, if machine learning were employed as a predictive tool to do this.\n",
    "\n",
    "[1] <i>Caddy, Brian, ed.  Forensic Examination of Glass and Paint.  London: Taylor & Francis, 2001, p. 61.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning the question around: If one were to subset only for containers, how would that dataset compare with the reference samples?  Both are containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container description:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ref_ix</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.518928</td>\n",
       "      <td>12.827692</td>\n",
       "      <td>0.773846</td>\n",
       "      <td>2.033846</td>\n",
       "      <td>72.366154</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>10.123846</td>\n",
       "      <td>0.187692</td>\n",
       "      <td>0.060769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.777037</td>\n",
       "      <td>0.999146</td>\n",
       "      <td>0.693920</td>\n",
       "      <td>1.282319</td>\n",
       "      <td>2.138695</td>\n",
       "      <td>2.183791</td>\n",
       "      <td>0.608251</td>\n",
       "      <td>0.155588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.513160</td>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>69.890000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>5.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.516660</td>\n",
       "      <td>12.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>72.180000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.519940</td>\n",
       "      <td>12.970000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>72.690000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>11.270000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.521190</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>73.390000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>11.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.523690</td>\n",
       "      <td>14.010000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>73.880000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ref_ix         Na         Mg         Al         Si          K  \\\n",
       "count  13.000000  13.000000  13.000000  13.000000  13.000000  13.000000   \n",
       "mean    1.518928  12.827692   0.773846   2.033846  72.366154   1.470000   \n",
       "std     0.003345   0.777037   0.999146   0.693920   1.282319   2.138695   \n",
       "min     1.513160  11.030000   0.000000   1.400000  69.890000   0.130000   \n",
       "25%     1.516660  12.730000   0.000000   1.560000  72.180000   0.380000   \n",
       "50%     1.519940  12.970000   0.000000   1.760000  72.690000   0.580000   \n",
       "75%     1.521190  13.270000   1.710000   2.170000  73.390000   0.970000   \n",
       "max     1.523690  14.010000   2.680000   3.500000  73.880000   6.210000   \n",
       "\n",
       "              Ca         Ba         Fe  \n",
       "count  13.000000  13.000000  13.000000  \n",
       "mean   10.123846   0.187692   0.060769  \n",
       "std     2.183791   0.608251   0.155588  \n",
       "min     5.870000   0.000000   0.000000  \n",
       "25%     9.700000   0.000000   0.000000  \n",
       "50%    11.270000   0.000000   0.000000  \n",
       "75%    11.530000   0.000000   0.000000  \n",
       "max    12.500000   2.200000   0.510000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mg : min to max: 0.0 to 2.68\n",
      "reference: Clear facsimile jar = 2.4 wt%\n",
      "  1930s Artifact jar= 4.3 wt%\n",
      "\n",
      "Ca : min to max: 5.87 to 12.5\n",
      "reference: Clear facsimile jar = 3.6 wt%\n",
      "  1930s Artifact jar= 8.5 wt%\n",
      "\n",
      "Ba : min to max: 0.0 to 2.2\n",
      "reference: Clear facsimile jar = 0.37 wt%\n",
      "  1930s Artifact jar= 0.74 wt%\n"
     ]
    }
   ],
   "source": [
    "sampcont = dataset[dataset['Type'].isin(['Container'])]\n",
    "sampcont=sampcont.drop(['ID', 'Type'], axis=1)\n",
    "print('Container description:')\n",
    "display(sampcont.describe())\n",
    "rangedict = {'Ba':['Barium',[.37,.74]],\n",
    "                 'Mg':['Magnesium',[2.4,4.3]],\n",
    "                 'Ca':['Calcium',[3.6,8.5]]}\n",
    "for element in ELEMENTS:\n",
    "    print()\n",
    "    print(\n",
    "        element,': min to max:',sampcont[element].min(),'to'\n",
    "        ,sampcont[element].max())\n",
    "\n",
    "    lowref=rangedict[element][1][0]\n",
    "    highref=rangedict[element][1][1]\n",
    "    print('reference: Clear facsimile jar =',lowref,'wt%'\n",
    "              '\\n  1930s Artifact jar=',highref,'wt%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a custom diverging colormap. Use the drop-down below this code to choose different cuts of the data, based on glass type, and their corresponding correlations interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a6c74b903d4cf89fff1dc5f524b183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='glass_type', options=('WindowF', 'WindowNF', 'VehicleF', 'Containe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "\n",
    "TYPES = ['WindowF', 'WindowNF', 'VehicleF', 'Container', \n",
    "         'Tableware', 'Headlamp']\n",
    "\n",
    "def get_dataset(src, glass_type):\n",
    "    #subset the df into a new df\n",
    "    df = src[src.Type == glass_type].copy()\n",
    "    df.drop(['ID'], axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "def make_heatmap(source, title):\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    corr=source.corr()\n",
    "    # Generate a mask for the lower triangle\n",
    "    mask = np.tril(np.ones_like(corr, dtype=bool))\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, annot=True, fmt=\".2f\",\n",
    "    vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(fontsize=14,rotation=0)\n",
    "    plt.yticks(fontsize=14,rotation=0)\n",
    "    plt.gcf().set_size_inches(15, 8)\n",
    "    return plt\n",
    "\n",
    "title='WindowF'\n",
    "def update_heatmap(glass_type):\n",
    "    plt.title.text = title\n",
    "    src = get_dataset(dataset, glass_type)\n",
    "    make_heatmap(src, glass_type + ' glass correlation')\n",
    "\n",
    "interact(update_heatmap, glass_type=TYPES);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "One might expect barium, an ingredient used to increase brilliance, thus increasing refractive index, would be positively correlated with refractive index in these graphs. Barium, however, is frequently absent in the 1987 dataset or present in only trace amounts. It may be that late 20th century glassmaking does not incorporate barium to the same degree that it was incorporated into glass production of the 1930s. See a 1936 Hazel Atlas glass patent [here](https://tighar.org/Projects/Earhart/Archives/Research/ResearchPapers/freckleintime/Document_05_FrancisFlint1936patent.pdf). Container glass negatively correlates barium with refractive index. Window non-float glass, however, shows a moderate correlation. This makes sense, since this type of glass is often found in churches or for specialized uses in which an aesthetic ambience is desired. See https://en.wikipedia.org/wiki/Crown_glass_(window)#/media/File:AKMuseum5.JPG as an example.\n",
    "\n",
    "With the exception of headlamps, calcium, probably in the form of calcium oxide, is highly correlated in the 1987 database with refractive index. This may be because of the impurities present in CaO. See https://digitalfire.com/oxide/cao. Therefore, calcium might have been the chosen element to increase brilliance in 1980s glass production, rather than barium. Price pressure might have played a role, but it is not known what the typical rates per ton were for these materials in 1987 or what quantity of CaO, relative to other choices, might be needed to be purchased in the recipe to achieve the desired brilliance of the glass.\n",
    "\n",
    "As Francis Flint described in his patent (cited above), the use of barium sulfate increased brilliance, but the sulfates needed then to be reduced to prevent small seeds forming in the glass mixture, thus reducing the quality of the glass. Flint recommended zinc, magnesium, aluminum or tin as reducing agents. Sodium and calcium have been recommended in more modern literature of the art [2]. In window non-float glass, aluminum seems to be jointly correlated with barium and refractive index. In tableware, aluminum seems to be jointly correlated with calcium and refractive index.\n",
    "\n",
    "It would seem good practice to analyze the correlations of each of these glass types separately, as we have done, since obviously the desired qualities of the glass will differ depending on the uses to which the glass will be put, and thus recipes will differ accordingly. The desired refractive index and brilliance of vehicle float glass will be far different than that of container or tableware glass, for example.\n",
    "\n",
    "The elemental correlations in this 1987 database suggest changing priorities between production techniques of the 1930s and production techniques of the 1980s, toward more utilitarian styles and techniques. One does not require statistics to observe that container glass resembling church windows has all but disappeared, if it ever was very common, giving way to containers in which seeing the contents clearly is the overriding concern. This change will almost certainly affect what specific types of glass that machine learning models can or cannot predict for these unseen samples.\n",
    "\n",
    "[2] <i>Kaur, Gurbinder. Bioactive Glasses: Potential Biomaterials for Future Therapy. Germany: Springer International Publishing, 2017, p. 107.<i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using machine learning to train a model on the 1987 database, can that model be used to identify the type (container) of one or both of the older samples unseen by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Mean Square Error results of different models\n",
      "LR: -1.221668 (0.401844)\n",
      "LASSO: -2.503540 (0.552316)\n",
      "EN: -2.124684 (0.546509)\n",
      "KNN: -0.952889 (0.511672)\n",
      "CART Regressor: -1.158932 (0.706080)\n",
      "SVR: -4.767522 (0.880490)\n",
      "KNClass: -1.626688 (1.013136)\n",
      "RandomForest: -1.170697 (0.850375)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Before SMOTE: Counter({2.0: 61, 1.0: 56, 7.0: 23, 3.0: 14, 5.0: 10, 6.0: 7})\n",
      "After SMOTE: Counter({1.0: 61, 7.0: 61, 3.0: 61, 2.0: 61, 6.0: 61, 5.0: 61})\n",
      "\n",
      "Random Forest without SMOTE oversampling\n",
      "[[12  2  0  0  0  0]\n",
      " [ 1 13  0  1  0  0]\n",
      " [ 2  0  1  0  0  0]\n",
      " [ 0  1  0  3  0  1]\n",
      " [ 0  1  0  0  1  0]\n",
      " [ 1  1  0  1  0  3]]\n",
      "Kappa (1=perfect;0=chance;<0=worse than chance): 0.634394\n",
      "MAD (Mean Absolute Dev): 0.711111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.86      0.80        14\n",
      "         2.0       0.72      0.87      0.79        15\n",
      "         3.0       1.00      0.33      0.50         3\n",
      "         5.0       0.60      0.60      0.60         5\n",
      "         6.0       1.00      0.50      0.67         2\n",
      "         7.0       0.75      0.50      0.60         6\n",
      "\n",
      "    accuracy                           0.73        45\n",
      "   macro avg       0.80      0.61      0.66        45\n",
      "weighted avg       0.75      0.73      0.72        45\n",
      "\n",
      "artifact prediction: WindowNF\n",
      "The artifact has a probability of:\n",
      "43% to be a WindowNF\n",
      "35% to be a WindowF\n",
      "16% to be a Headlamp\n",
      "6% to be a VehicleF\n",
      "0% to be a Container\n",
      "0% to be a Tableware\n",
      "\n",
      "clear facsimile prediction: Headlamp\n",
      "The clear facsimile has a probability of:\n",
      "37% to be a Headlamp\n",
      "22% to be a Container\n",
      "15% to be a WindowF\n",
      "15% to be a WindowNF\n",
      "8% to be a Tableware\n",
      "3% to be a VehicleF\n",
      "\n",
      "Random Forest with SMOTE oversampling\n",
      "[[13  1  0  0  0  0]\n",
      " [ 1 13  0  1  0  0]\n",
      " [ 1  0  2  0  0  0]\n",
      " [ 1  0  0  4  0  0]\n",
      " [ 0  0  0  0  2  0]\n",
      " [ 2  0  0  1  0  3]]\n",
      "Kappa (1=perfect;0=chance;<0=worse than chance): 0.760797\n",
      "MAD (Mean Absolute Dev): 0.555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.93      0.81        14\n",
      "         2.0       0.93      0.87      0.90        15\n",
      "         3.0       1.00      0.67      0.80         3\n",
      "         5.0       0.67      0.80      0.73         5\n",
      "         6.0       1.00      1.00      1.00         2\n",
      "         7.0       1.00      0.50      0.67         6\n",
      "\n",
      "    accuracy                           0.82        45\n",
      "   macro avg       0.89      0.79      0.82        45\n",
      "weighted avg       0.85      0.82      0.82        45\n",
      "\n",
      "[[0.42 0.4  0.07 0.04 0.   0.07]]\n",
      "artifact prediction: WindowF\n",
      "The artifact has a probability of:\n",
      "42% to be a WindowF\n",
      "40% to be a WindowNF\n",
      "7% to be a VehicleF\n",
      "7% to be a Headlamp\n",
      "4% to be a Container\n",
      "0% to be a Tableware\n",
      "\n",
      "clear facsimile prediction: Container\n",
      "The clear facsimile has a probability of:\n",
      "46% to be a Container\n",
      "31% to be a WindowNF\n",
      "14% to be a WindowF\n",
      "8% to be a Headlamp\n",
      "1% to be a Tableware\n",
      "0% to be a VehicleF\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "def make_nicer_probability_output(array_to_convert,title):\n",
    "    \"\"\"\n",
    "    Author: Joe Cerniglia\n",
    "    Date: March 20, 2022\n",
    "    A function to convert the standard probability scores in Python machine learning libraries\n",
    "    to a report that supplies the categories from a dictionary and sorts the list of probabilities in\n",
    "    descending order.\n",
    "    \n",
    "    The standard probability scores supplied by the predict_proba function appear like this:\n",
    "    \n",
    "    [[0.42 0.4  0.07 0.04 0.   0.07]]\n",
    "    \n",
    "    In their stead, I would like an equivalent and more readable report that formats the 2d list to \n",
    "    appear like this:\n",
    "    \n",
    "    The artifact has a probability of:\n",
    "    42% to be a WindowF\n",
    "    40% to be a WindowNF\n",
    "    7% to be a VehicleF\n",
    "    7% to be a Headlamp\n",
    "    4% to be a Container\n",
    "    0% to be a Tableware\n",
    "\n",
    "    Parameters:\n",
    "    array_to_convert: an array to convert to a sorted 2d list \n",
    "    title: some text to be placed in the title or headings of the report\n",
    "    Preconditions:\n",
    "    The array must have the exact number of elements and format needed for the dictionary\n",
    "    and must be the output of a call to model.predict_proba\n",
    "    \n",
    "    returns None. The function itself prints the report.\n",
    "    \"\"\"\n",
    "    prob_list = array_to_convert.tolist()\n",
    "    # make a 2d list out of the 1d list\n",
    "    # The 2d list will include a counter variable so that \n",
    "    # the dictionary lookup integer key of glass type will \n",
    "    # not be dependent on an index for its value,\n",
    "    # thus allowing a sort and then retrieval of the key\n",
    "    counter=0\n",
    "    for probability in prob_list[0]:\n",
    "        prob_list[0][counter]=[counter,probability]\n",
    "        counter+=1\n",
    "    # We have a 3d list; get back to the 2d list\n",
    "    prob_list=prob_list[0]\n",
    "    # Sort in descending order the second column of each row in the 2d list\n",
    "    # This allows for a descending order of probability and for the\n",
    "    # predicted type to rise to the top of the list\n",
    "    prob_list=sorted(prob_list,key=lambda l:l[1], reverse=True)\n",
    "\n",
    "    counter=0\n",
    "    for prediction in prob_list:\n",
    "        if counter==0:\n",
    "            print('The ' + title + ' has a probability of:')\n",
    "        print(\"{:.0%}\".format(prediction[1]),'to be a',probdict[prediction[0]])\n",
    "        counter+=1\n",
    "    return None\n",
    "\n",
    "\n",
    "array = dataset_ml.values\n",
    "X = array[:,1:10]\n",
    "Y = array[:,10]\n",
    "\n",
    "\n",
    "\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n",
    "    test_size=validation_size, random_state=seed, stratify=Y)\n",
    "\n",
    "#names=\n",
    "#  ['Ref_ix','Na',  'Mg',   'Al', 'Si',  'K',  'Ca',  'Ba',  'Fe','Type']\n",
    "#random headlamp=\n",
    "    #[1.51602,  14.85, 0.00,  2.38, 73.28, 0.00, 8.76, 0.64, 0.09, 7]\n",
    "# row equals the 1930s artifact jar.\n",
    "# row2 equals the clear facsimile jar of unknown date\n",
    "row=[1.52369,  13.1,  4.3,  .74,   72.37,  .24,  8.5,  .74,  .02]\n",
    "row2=[1.51316, 11.7,  2.4,  .85,   72.37,  .12,  3.6,  .37,  .01]\n",
    "row_array=np.array(row).reshape(1,9)\n",
    "row_array2=np.array(row2).reshape(1,9)\n",
    "#print('shape:',X_validation.shape)\n",
    "#print('shape:',row_array.shape)\n",
    "X_validation=np.append(\n",
    "X_validation,row_array).reshape(X_validation.shape[0]+1,9)\n",
    "X_validation=np.append(\n",
    "X_validation,row_array2).reshape(X_validation.shape[0]+1,9)\n",
    "\n",
    "#print('shape:',X_validation.shape)\n",
    "#print('shape:',X_train.shape)\n",
    "\n",
    "rowval=[5.0]\n",
    "rowval2=[5.0]\n",
    "row_array=np.array(rowval).reshape(1,1)\n",
    "row_array2=np.array(rowval2).reshape(1,1)\n",
    "\n",
    "Y_validation=np.append(\n",
    "Y_validation,row_array).reshape(Y_validation.shape[0]+1,1)\n",
    "\n",
    "Y_validation=np.append(\n",
    "Y_validation,row_array2).reshape(Y_validation.shape[0]+1,1)\n",
    "\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('LASSO', Lasso()))\n",
    "models.append(('EN', ElasticNet()))\n",
    "models.append(('KNN', KNeighborsRegressor(n_neighbors=5)))\n",
    "models.append(('CART Regressor', DecisionTreeRegressor()))\n",
    "models.append(('SVR', SVR()))\n",
    "models.append(('KNClass',KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('RandomForest',RandomForestClassifier(\n",
    "n_estimators=100, max_features=9,class_weight='balanced')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "print('Evaluation of Mean Square Error results of different models')\n",
    "for name, model in models:\n",
    "  kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=7)\n",
    "  cv_results = cross_val_score(\n",
    "  model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "\n",
    " # Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('UNTUNED Unscaled Algorithm Comparison',fontsize=20)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.gcf().set_size_inches(10, 8)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "probdict = {0: 'WindowF',\n",
    "  1: 'WindowNF',\n",
    "  2: 'VehicleF',\n",
    "  3: 'Container',\n",
    "  4: 'Tableware',\n",
    "  5: 'Headlamp'}\n",
    "\n",
    "print()\n",
    "print()\n",
    "counter=Counter(Y_train)\n",
    "print('Before SMOTE:',counter)\n",
    "\n",
    "oversample = SMOTE(random_state=42,k_neighbors=5)\n",
    "X_trainsm, Y_trainsm = oversample.fit_resample(X_train, Y_train)\n",
    "counter=Counter(Y_trainsm)\n",
    "print('After SMOTE:',counter)\n",
    "\n",
    "modelnoSMOTE=make_pipeline(\n",
    "StandardScaler(),RandomForestClassifier(\n",
    "n_estimators=100,max_features=9,class_weight='balanced'))\n",
    "\n",
    "\n",
    "modelwithSMOTE=make_pipeline(\n",
    "StandardScaler(),SMOTE(\n",
    "random_state=42, k_neighbors=5),RandomForestClassifier(\n",
    "n_estimators=100,max_features=9,class_weight='balanced'))\n",
    "\n",
    "\n",
    "modelnoSMOTE.fit(X_train, Y_train)\n",
    "\n",
    "print()\n",
    "print('Random Forest without SMOTE oversampling')\n",
    "predictions = modelnoSMOTE.predict(X_validation)\n",
    "ytrue=5\n",
    "predictions=np.round(predictions, 0)\n",
    "\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "kappa=cohen_kappa_score(Y_validation, np.round(predictions,0))\n",
    "MAD = mean_absolute_error(Y_validation, predictions)\n",
    "print('Kappa (1=perfect;0=chance;<0=worse than chance): %f' % kappa)\n",
    "print('MAD (Mean Absolute Dev): %f' % MAD)\n",
    "print(classification_report(Y_validation, predictions, zero_division=1))\n",
    "yhat = modelnoSMOTE.predict([row])\n",
    "yhat2 = modelnoSMOTE.predict([row2])\n",
    "yhat_probability = modelnoSMOTE.predict_proba([row])\n",
    "yhat2_probability = modelnoSMOTE.predict_proba([row2])\n",
    "\n",
    "print('artifact prediction:',dict[np.round(yhat[0])])\n",
    "make_nicer_probability_output(yhat_probability,'artifact')\n",
    "print()\n",
    "print('clear facsimile prediction:',dict[np.round(yhat2[0])])\n",
    "make_nicer_probability_output(yhat2_probability,'clear facsimile')\n",
    "\n",
    "\n",
    "modelwithSMOTE.fit(X_train, Y_train)\n",
    "\n",
    "print()\n",
    "print('Random Forest with SMOTE oversampling')\n",
    "predictions = modelwithSMOTE.predict(X_validation)\n",
    "ytrue=5\n",
    "predictions=np.round(predictions, 0)\n",
    "\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "kappa=cohen_kappa_score(Y_validation, np.round(predictions,0))\n",
    "MAD = mean_absolute_error(Y_validation, predictions)\n",
    "print('Kappa (1=perfect;0=chance;<0=worse than chance): %f' % kappa)\n",
    "print('MAD (Mean Absolute Dev): %f' % MAD)\n",
    "print(classification_report(Y_validation, predictions, zero_division=1))\n",
    "yhat = modelwithSMOTE.predict([row])\n",
    "yhat2 = modelwithSMOTE.predict([row2])\n",
    "yhat_probability = modelwithSMOTE.predict_proba([row])\n",
    "yhat2_probability = modelwithSMOTE.predict_proba([row2])   \n",
    "\n",
    "print('artifact prediction:',dict[np.round(yhat[0])])\n",
    "make_nicer_probability_output(yhat_probability,'artifact')\n",
    "print()\n",
    "print('clear facsimile prediction:',dict[np.round(yhat2[0])])\n",
    "make_nicer_probability_output(yhat2_probability,'clear facsimile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Procedure\n",
    "The following sequence of operations was performed:<br>\n",
    "1) Create two arrays. The first contains the database features, all of the predictive columns of the original dataset. The second contains the target column, Type.<br>\n",
    "2) Create a train-test, stratified split so that the model can be trained on 80% of the data and validated on a subsample of 20%.<br>\n",
    "3) Add the 2 exogenous glass sample jars to the validation (unseen) data, both for features (X) and for target (Y).<br>\n",
    "4) Test a set of regression and classification models on the training dataset and report the MSE (mean squared error) results for each one, as well as the standard deviation of the MSE.<br>\n",
    "5) Plot the preliminary effectiveness of each model on a box plot.<br>\n",
    "6) Select the model most likely to succeed. In this case, Random Forest Classifier was selected. Use the standard scaler to make the scale of the features more uniform with one another, and a class weight of 'balanced' to minimize the imbalance in the glass type categories.<br> \n",
    "7) Fit the model to the dataset.<br>\n",
    "8) Print a report on the performance of the model.<br>\n",
    "9) Print a report on the predicted result for both of the two unseen jars and the probability of the prediction for each glass type.<br>\n",
    "\n",
    "### Findings\n",
    "One of the most interesting findings was that when the SMOTE oversampling algorithm was applied to the data pipeline prior to applying the model, the resolving power of the model increased to the point that it was consistently able to identify the clear facsimile jar correctly as a container. Without the SMOTE oversampling algorithm applied, the model predicted the clear facsimile jar to be a headlamp. The prediction of container scored a distant third or fourth place.\n",
    "\n",
    "The non-SMOTE model predicted the 1930s artifact jar to be a window, usually of the float variety*. This is very close to the prediction made by the model when SMOTE was used. Following application of SMOTE oversampling, the model predicted the artifact jar to be a non-float window.\n",
    "\n",
    "These machine learning model predictions would seem to indicate that the clear facsimile jar of unknown date can be correctly identified through prudent enhancement of the model with oversampling techniques. These techniques, however, are ineffective in increasing the ability of the model to predict the 1930s artifact jar to be what it is, a container.\n",
    "\n",
    "Using a well-tuned model, the fact that a non-float window, of a type often typically seen in churches, could be predicted for the 1930s artifact jar demonstrates that this database of late 20th century glassware lacks enough relevant case examples that would allow it to predict this artifact correctly.\n",
    "\n",
    "### Conclusions\n",
    "But could these case examples be found? Our machine learning model can correctly characterize the clear facsimile, a jar of the same size and shape as the artifact, and possibly from the same era. The model cannot correctly characterize the semi-opaque 1930s artifact jar. One might suppose that the 1930s artifact jar has a rare and original recipe that, in the absence of any information other than the data nourishing the model, is not readily identifiable as to the glass type, even with the powerful machine learning tools, such as random forests, available to us in 2022. This fact is further confirmation of the artifact jar's originality, perhaps even in its own time.\n",
    "\n",
    "\\* Because the model is stochastic in nature, results will not be exactly the same each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
